# -*- coding: utf-8 -*-
"""flight fare prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1O9NsVSRIX7pV9KBsdPfv_VR5Jzfd-o7j

##IMPORTING DATASET
"""

#importing pandas to read dataset from drive9
import pandas as pd
#the dataset read is stored as dataframe in df variable
data_frame=pd.read_excel('/content/flight_fare.xlsx')
#gives the number of rows and columns in the dataset
data_frame.shape

#displays the first 5 rows in the dataset
data_frame.head()

#gives info about the dataset N
data_frame.info()
#ggeting the names of each features
data_frame.columns

"""##CLEANING AND PREPROCESSING




"""

#drops the null value in the dataset
data_frame.dropna(inplace=True)
data_frame.isnull().sum()

#finding the types of duration
data_frame['Duration'].value_counts()

#separating date and month for easy access
data_frame['journy_date']=pd.to_datetime(data_frame['Date_of_Journey'],format= "%d/%m/%Y").dt.day
data_frame['journy_month']=pd.to_datetime(data_frame['Date_of_Journey'], format ="%d/%m/%Y").dt.month
data_frame.drop(['Date_of_Journey'],axis=1,inplace=True)

data_frame.head()

#separating departure hour and minute as separate features
data_frame['dep_hour']=pd.to_datetime(data_frame['Dep_Time']).dt.hour
data_frame['dep_minute']=pd.to_datetime(data_frame['Dep_Time']).dt.minute
data_frame.drop(['Dep_Time'],axis=1,inplace=True)

data_frame.head()

#separating arrival hour and minute as separate columns
data_frame['arrival_hour']=pd.to_datetime(data_frame['Arrival_Time']).dt.hour
data_frame['arrival_minute']=pd.to_datetime(data_frame['Arrival_Time']).dt.minute
data_frame.drop(['Arrival_Time'],axis=1,inplace=True)

data_frame.head()

time_interval=list(data_frame["Duration"]) #it will cover the duration column to list
for i in range(len(time_interval)):
  if len(time_interval[i].split())!=2:
    if 'h' in time_interval[i]: 
      time_interval[i]=time_interval[i].strip()+" 0m" #changing the string to standard format "00h 00m"
    else:
      time_interval[i]="0h " +time_interval[i]     #changing the string to standard format "00h 00m"
       

duration_hr=[] #this will store the hours from duration
duration_min=[] #this will store the minutes from duration
for i in range(len(time_interval)):
  duration_hr.append(int(time_interval[i].split(sep= "h")[0]))
  duration_min.append(int(time_interval[i].split(sep= "m")[0].split()[-1]))

#separating duration hour and minute as separate columns 
data_frame["duration_hour"]=duration_hr
data_frame["duration_minute"]=duration_min
data_frame.drop(["Duration"],axis=1,inplace=True)

data_frame.head()

#finding the unique values in the feature airlines
data_frame["Airline"].value_counts()

import seaborn as sns 
sns.catplot(y="Price",x="Airline",data=data_frame.sort_values("Price",ascending=False),kind="boxen",height=6,aspect=3)

"""This graph shows the Price difference with respect to the various airlines.This is used to find the outliers in the feature airlines"""

airline=data_frame["Airline"]
airline=pd.get_dummies(airline,drop_first=True)
airline.head()

#finding the uniquie values from the feature source
data_frame["Source"].value_counts()

sns.catplot(y="Price",x="Source",data=data_frame.sort_values("Price",ascending=False),kind="boxen",height=6,aspect=3)

"""This graph shows the Price difference with respect to the departure of the airline.This is used to find the outliers in the feature source """

#performing one-hot encoding
data_source=data_frame["Source"]
data_source=pd.get_dummies(data_source,drop_first=True)
data_source.head()

#finding the uniquie values from the feature destination
data_frame["Destination"].value_counts()

sns.boxplot(y="Price",x="Destination",data=data_frame.sort_values("Price",ascending=False))

"""This plot gives the most visited destination with respect to price of the airlines"""

#performing one-hot encoding
data_destination=data_frame["Destination"]
data_destination=pd.get_dummies(data_destination,drop_first=True)
data_destination.head()

data_frame.drop(["Route","Additional_Info"],axis=1,inplace=True)

#finding the uniquie values from the feature total stops
data_frame["Total_Stops"].value_counts()

data_frame.replace({"non-stop":0,"1 stop":1,"2 stops":2,"3 stops":3,"4 stops":4},inplace=True)

graph= sns.relplot(x="Airline", y="Price", kind="line", data=data_frame)
graph.fig.autofmt_xdate()

"""From the above graph the relation between various airlines based on the price details is infered.The jet airways business airlines is the highest compared to other airlines."""

sns.set(style="ticks", color_codes=True)
sns.catplot(x="Destination", y="Price", data=data_frame);

"""From the above graph the relation between various airlines based on the price details and destination of the flight is infered.The travel to New Delhi costs more than the other destinations."""

sns.relplot(x="Source", y="Destination", hue="Price", data=data_frame);

"""From the above graph the relation between various airlines based on the source and destination and derived the price of the flight.The travel to New Delhi from Banglore costs more than the other travels.

SPLITTING TRAIN AND TEST DATA
"""

Train_Data=pd.concat([data_frame,airline,data_destination,data_source],axis=1)
Train_Data.head()

Train_Data.drop(['Airline','Source','Destination'],axis=1,inplace=True)
Train_Data.head()
Train_Data.shape

Train_Data.head()

Test_Data=pd.read_excel('/content/Test_set.xlsx')
Test_Data.head()

"""Performing feature engineering for the test data seperately to avoid data leakage"""

print("Information about test data")
print(Test_Data.info())
print()
print()
print("."*100)
print("Going through the Null values :")

Test_Data.dropna(inplace = True)
print(Test_Data.isnull().sum())

# Date_of_Journey
Test_Data["Journey_day"] =pd.to_datetime(Test_Data['Date_of_Journey'],format= "%d/%m/%Y").dt.day
Test_Data["Journey_month"] = pd.to_datetime(Test_Data["Date_of_Journey"], format = "%d/%m/%Y").dt.month
Test_Data.drop(["Date_of_Journey"], axis = 1, inplace = True)

# Dep_Time
Test_Data["Dep_hour"] = pd.to_datetime(Test_Data["Dep_Time"]).dt.hour
Test_Data["Dep_min"] = pd.to_datetime(Test_Data["Dep_Time"]).dt.minute
Test_Data.drop(["Dep_Time"], axis = 1, inplace = True)

# Arrival_Time
Test_Data["Arrival_hour"] = pd.to_datetime(Test_Data.Arrival_Time).dt.hour
Test_Data["Arrival_min"] = pd.to_datetime(Test_Data.Arrival_Time).dt.minute
Test_Data.drop(["Arrival_Time"], axis = 1, inplace = True)

# Duration
duration = list(Test_Data["Duration"])

for i in range(len(duration)):
    if len(duration[i].split()) != 2:    # Check if duration contains only hour or mins
        if "h" in duration[i]:
            duration[i] = duration[i].strip() + " 0m"   # Adds 0 minute
        else:
            duration[i] = "0h " + duration[i]           # Adds 0 hour

duration_hours = []
duration_mins = []
for i in range(len(duration)):
    duration_hours.append(int(duration[i].split(sep = "h")[0]))    # Extract hours from duration
    duration_mins.append(int(duration[i].split(sep = "m")[0].split()[-1]))   # Extracts only minutes from duration
# Adding Duration column to test set
Test_Data["Duration_hours"] = duration_hours
Test_Data["Duration_mins"] = duration_mins
Test_Data.drop(["Duration"], axis = 1, inplace = True)


# Going through the Categorical Features
print("."*100)
print(" going through the Airline")

print(Test_Data["Airline"].value_counts())
Airline = pd.get_dummies(Test_Data["Airline"], drop_first= True)

print()

print("Source")
print("."*75)
print(Test_Data["Source"].value_counts())
Source = pd.get_dummies(Test_Data["Source"], drop_first= True)

print()

print("Going through the Destination")
print("."*100)
print(Test_Data["Destination"].value_counts())
Destination = pd.get_dummies(Test_Data["Destination"], drop_first = True)

# Additional_Info contains almost 80% no_info
# Route and Total_Stops are related to each other
Test_Data.drop(["Route", "Additional_Info"], axis = 1, inplace = True)

# Replacing Total_Stops
Test_Data.replace({"non-stop": 0, "1 stop": 1, "2 stops": 2, "3 stops": 3, "4 stops": 4}, inplace = True)
# Concatenate dataframe --> data_test + Airline + Source + Destination
Test_Data = pd.concat([Test_Data, Airline, Source, Destination], axis = 1)

Test_Data.drop(["Airline", "Source", "Destination"], axis = 1, inplace = True)

print()
print()

print("Shape of test data : ", Test_Data.shape)

#spliting the x indendent and dependent variables from the dataset
Train_Data.columns
x=Train_Data.loc[:,['Total_Stops','journy_date', 'journy_month', 'dep_hour',
       'dep_minute', 'arrival_hour', 'arrival_minute', 'duration_hour',
       'duration_minute', 'Air India', 'GoAir', 'IndiGo', 'Jet Airways',
       'Jet Airways Business', 'Multiple carriers',
       'Multiple carriers Premium economy', 'SpiceJet', 'Trujet', 'Vistara',
       'Vistara Premium economy', 'Cochin', 'Delhi', 'Hyderabad', 'Kolkata',
       'New Delhi', 'Chennai', 'Delhi', 'Kolkata', 'Mumbai']]
x.head()
y=Train_Data.iloc[:,1]
y.head()

#Finding the correlation with the features
import matplotlib.pyplot as plt
plt.figure(figsize=(18,18 ))
sns.heatmap(data_frame.corr(),annot=True,cmap="RdYlGn")

"""This heatmap shows the relationship between differnt features

Spliting the test and train data
"""

#splting train and test data with train size as 70% and test size as 30% with random state 42
from sklearn.model_selection import train_test_split 
x_train,x_test,y_train,y_test=train_test_split (x,y,test_size=0.3,random_state=42)

"""### Training the model with KNN"""

from sklearn.metrics import r2_score,mean_absolute_error,mean_squared_error,recall_score
r2_scores = {}

def fit_and_evaluate(prediction_model):
    print(' Training the dataset with KNN algorithm')
    
    model= prediction_model.fit(x_train,y_train)
    print("Model Accuracy: {}".format(model.score(x_train,y_train)))
    

    predictions = model.predict(x_test)
    print("Predictions:\n",predictions)
    
    print('\n')
    
    r2score=r2_score(y_test,predictions) 
    print("r2 score is: {}".format(r2score))
    r2_scores[f'{prediction_model}'] = r2score
          
    print('MAE:{}'.format(mean_absolute_error(y_test,predictions)))
    print('MSE:{}'.format(mean_squared_error(y_test,predictions)))
    print('RMSE:{}'.format(np.sqrt(mean_squared_error(y_test,predictions))))

#import required packages
from sklearn import neighbors
from sklearn.metrics import mean_squared_error 
from math import sqrt

#list which stores RMSE values for various K values 
accuracy = [] 

for K in range(20):
    K = K+1
    model = neighbors.KNeighborsRegressor(n_neighbors = K)

    model.fit(x_train, y_train)  #fit the model
    pred=model.predict(x_test)
     #make prediction on test dataset
    accu= model.score(x_test,y_test) #calculate accuracy
    accuracy.append(accu) #store rmse values
    print('accuracy value for k= ' , K , 'is:', accu)
print(accuracy)

#plotting the accuracy against k values
curve = pd.DataFrame(accuracy) #elbow curve 
curve.plot()

"""This graph shows the differnt accuracy with respect to differnt K values"""

from sklearn import metrics
import numpy as np
print('MAE:', metrics.mean_absolute_error(y_test, pred))
print('MSE:', metrics.mean_squared_error(y_test, pred))
print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, pred)))

"""## decision tree"""

from sklearn import tree
from sklearn.metrics import accuracy_score

#fitting th data with decision tree regressor algorithm
Decision_tree=tree.DecisionTreeRegressor()
Decision_tree.fit(x_train,y_train)

#predicting the score of the model
pred = Decision_tree.predict(x_test)
Decision_tree.score(x_test,y_test)

from sklearn import metrics
print('MAE:', metrics.mean_absolute_error(y_test, pred))
print('MSE:', metrics.mean_squared_error(y_test, pred))
print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, pred)))

from yellowbrick.model_selection import ValidationCurve
from sklearn.tree import DecisionTreeRegressor
#drawing the validation curve
val_curv = ValidationCurve(
    DecisionTreeRegressor(), param_name="max_depth",
    param_range=np.arange(1, 11), cv=10, scoring="r2"
)

# Fit and show the visualizer
val_curv.fit(x_test,y_test)
val_curv.show()

"""This Plot shows the validation curve for DecisionTree Regressor

## Random Forest Regressor
"""

from sklearn.ensemble import ExtraTreesRegressor
selection=ExtraTreesRegressor()
selection.fit(x,y)
print(selection.feature_importances_)

#plot graph of feature importances for better visualization

plt.figure(figsize = (12,8))
important_feature = pd.Series(selection.feature_importances_, index=x.columns)
important_feature.nlargest(20).plot(kind='barh')
plt.show()

"""This graph shows the importance of differnt features."""

# fitting the data with random forest regressor
from sklearn.ensemble import RandomForestRegressor
reg_rf = RandomForestRegressor()
reg_rf.fit(x_train, y_train)

"""Performing HyperParameter tuning to obtain high accuracy"""

import numpy as np
from sklearn.model_selection import RandomizedSearchCV
#Randomized Search CV

# Number of trees in random forest
n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]
# Number of features to consider at every split
max_features = ['auto', 'sqrt']
# Maximum number of levels in tree
max_depth = [int(x) for x in np.linspace(5, 30, num = 6)]
# Minimum number of samples required to split a node
min_samples_split = [2, 5, 10, 15, 100]
# Minimum number of samples required at each leaf node
min_samples_leaf = [1, 2, 5, 10]
random_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf}
rf_random = RandomizedSearchCV(estimator = reg_rf, param_distributions = random_grid,scoring='neg_mean_squared_error', n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)
rf_random.fit(x_train,y_train)

#accessing the best parameters
rf_random.best_params_

#training the model with best parameters
from sklearn.ensemble import RandomForestRegressor
reg_rf_hyp = RandomForestRegressor(max_depth=20,max_features='auto',min_samples_leaf=15,n_estimators=700)
reg_rf_hyp.fit(x_train, y_train)

prediction =reg_rf_hyp.predict(x_test)

#predicting the score of the model
y_pred = reg_rf_hyp.predict(x_test)
reg_rf_hyp.score(x_train,y_train)
reg_rf_hyp.score(x_test,y_test)

plt.figure(figsize = (8,8))
sns.distplot(y_test-prediction)
plt.show()

from sklearn import metrics
print('MAE:', metrics.mean_absolute_error(y_test, prediction))
print('MSE:', metrics.mean_squared_error(y_test, prediction))
print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, prediction)))